{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9332113-653a-4579-8dfe-3bc467819dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae80bed-0c68-4458-b5c3-5a3b6806840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a custom standardization function to mainly strip tags \n",
    "def custom_standardization(input_data):\n",
    "    input_data = input_data.lower()\n",
    "    input_data = re.sub(r'^.*?<p>', '<p>', input_data)\n",
    "    input_data = re.sub(r'<p>', '', input_data)\n",
    "    input_data = re.sub(r'</p>', '', input_data)\n",
    "    input_data = re.sub(r'</a>', '', input_data)\n",
    "    input_data = re.sub(r'.*\\<(.*)\\>.*', '<a>', input_data)\n",
    "    input_data = re.sub(r'<a>', '', input_data)\n",
    "    #input_data = re.sub(r'\\n', '', input_data)\n",
    "    input_data = re.sub(r'<pre><code>', '', input_data)\n",
    "    input_data = re.sub(r'</code></pre>', '', input_data)\n",
    "    input_data = re.sub(r'<br>', '', input_data)\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467e4965-b466-4af8-9af4-4474c2c6c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the preprocess_documents function in gensim automatically stem\n",
    "# which is not desired\n",
    "# thus this customized gensim_strip function\n",
    "\n",
    "#from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "\n",
    "def gensim_strip(input_data):\n",
    "    input_data = input_data.lower()\n",
    "    input_data = strip_tags(input_data)\n",
    "    input_data = strip_short(input_data, minsize = 3)\n",
    "    input_data = strip_punctuation(input_data)\n",
    "    input_data = strip_numeric(input_data)\n",
    "    input_data = strip_non_alphanum(input_data)\n",
    "    input_data = strip_multiple_whitespaces(input_data)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9fe4d6-54a3-42e8-814b-70e6143edd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f242e516-d357-4b5f-836e-e8180048c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data and store the cleaned ones in \"./stackoverflow.txt\"\n",
    "\n",
    "f = open(os.path.join(\"./\", \"stackoverflow.txt\"), 'w', encoding='utf-8')\n",
    "for line in open('Answers.csv', encoding = \"ISO-8859-1\"):\n",
    "    line = gensim_strip(line)\n",
    "    doc = nlp(line)\n",
    "    line = \" \".join([token.lemma_ for token in doc])\n",
    "    out = strip_short(line, minsize = 3)\n",
    "    if out:\n",
    "        f.write(f\"{out}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3cfa4e-2deb-433c-a4d3-1e7b626acf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'access', 'account', 'action', 'activity', 'actually',\n",
       "       'add', 'address', 'ajax', 'alert', 'allow', 'amp', 'android',\n",
       "       'answer', 'api', 'app', 'append', 'application', 'apply',\n",
       "       'approach', 'args', 'argument', 'arr', 'array', 'article', 'ask',\n",
       "       'asp', 'assign', 'assume', 'attr', 'attribute', 'auto',\n",
       "       'available', 'avoid', 'background', 'bar', 'base', 'begin', 'bind',\n",
       "       'bit', 'block', 'body', 'bool', 'boolean', 'border', 'box',\n",
       "       'break', 'browser', 'buffer', 'build', 'button', 'byte', 'cache',\n",
       "       'callback', 'case', 'cast', 'catch', 'category', 'cause', 'cell',\n",
       "       'center', 'change', 'char', 'character', 'check', 'child', 'class',\n",
       "       'clear', 'click', 'client', 'close', 'code', 'col', 'collection',\n",
       "       'color', 'column', 'com', 'come', 'command', 'comment', 'common',\n",
       "       'compare', 'compile', 'compiler', 'complete', 'component',\n",
       "       'condition', 'config', 'configuration', 'connect', 'connection',\n",
       "       'consider', 'console', 'const', 'constructor', 'contain',\n",
       "       'container', 'content', 'context', 'control', 'controller',\n",
       "       'convert', 'copy', 'correct', 'count', 'course', 'create', 'css',\n",
       "       'current', 'custom', 'data', 'database', 'date', 'datetime',\n",
       "       'datum', 'day', 'debug', 'declare', 'def', 'default', 'define',\n",
       "       'delete', 'depend', 'dependency', 'design', 'developer', 'device',\n",
       "       'difference', 'different', 'dim', 'directly', 'directory',\n",
       "       'display', 'div', 'doc', 'document', 'documentation', 'doesn',\n",
       "       'domain', 'don', 'double', 'download', 'easy', 'echo', 'edit',\n",
       "       'element', 'email', 'enable', 'encode', 'end', 'enter', 'entity',\n",
       "       'entry', 'environment', 'equal', 'error', 'event', 'exactly',\n",
       "       'example', 'exception', 'execute', 'exist', 'expect', 'expression',\n",
       "       'extend', 'extension', 'fail', 'false', 'far', 'fast', 'feature',\n",
       "       'field', 'file', 'filename', 'filter', 'final', 'fine', 'fix',\n",
       "       'flag', 'float', 'folder', 'follow', 'font', 'foo', 'foreach',\n",
       "       'form', 'format', 'frame', 'framework', 'function', 'generate',\n",
       "       'git', 'github', 'global', 'good', 'google', 'grid', 'group',\n",
       "       'guess', 'handle', 'handler', 'happen', 'hash', 'head', 'header',\n",
       "       'height', 'help', 'hide', 'hope', 'host', 'href', 'html', 'http',\n",
       "       'https', 'idea', 'image', 'img', 'implement', 'implementation',\n",
       "       'import', 'include', 'index', 'info', 'information', 'init',\n",
       "       'initialize', 'input', 'insert', 'inside', 'install', 'instance',\n",
       "       'instead', 'int', 'integer', 'intent', 'interface', 'isn', 'issue',\n",
       "       'item', 'java', 'javascript', 'join', 'jquery', 'jsfiddle', 'json',\n",
       "       'just', 'key', 'know', 'label', 'language', 'large', 'layout',\n",
       "       'leave', 'length', 'let', 'level', 'library', 'like', 'limit',\n",
       "       'line', 'link', 'list', 'load', 'local', 'location', 'log',\n",
       "       'login', 'long', 'look', 'loop', 'lot', 'main', 'make', 'map',\n",
       "       'margin', 'match', 'max', 'maybe', 'mean', 'member', 'memory',\n",
       "       'mention', 'menu', 'message', 'method', 'microsoft', 'min', 'mode',\n",
       "       'model', 'modify', 'module', 'month', 'multiple', 'mysql', 'need',\n",
       "       'net', 'new', 'nil', 'node', 'non', 'note', 'null', 'num',\n",
       "       'number', 'obj', 'object', 'old', 'open', 'operation', 'operator',\n",
       "       'option', 'order', 'org', 'original', 'output', 'override',\n",
       "       'package', 'page', 'param', 'parameter', 'parent', 'parse', 'pass',\n",
       "       'password', 'path', 'pattern', 'perform', 'performance', 'php',\n",
       "       'place', 'plugin', 'point', 'pointer', 'port', 'position',\n",
       "       'possible', 'post', 'print', 'println', 'private', 'probably',\n",
       "       'problem', 'process', 'product', 'program', 'project', 'property',\n",
       "       'protect', 'provide', 'public', 'push', 'python', 'query',\n",
       "       'question', 'quote', 'random', 'range', 'read', 'really', 'reason',\n",
       "       'recommend', 'record', 'reference', 'regex', 'release', 'remove',\n",
       "       'render', 'replace', 'request', 'require', 'resource', 'response',\n",
       "       'result', 'return', 'right', 'root', 'route', 'row', 'rule', 'run',\n",
       "       'sample', 'save', 'say', 'scope', 'screen', 'script', 'search',\n",
       "       'second', 'section', 'select', 'self', 'send', 'separate',\n",
       "       'server', 'service', 'session', 'set', 'setting', 'share',\n",
       "       'similar', 'simple', 'simply', 'single', 'site', 'size', 'small',\n",
       "       'solution', 'solve', 'sort', 'source', 'space', 'span', 'specific',\n",
       "       'specify', 'split', 'sql', 'src', 'stack', 'standard', 'start',\n",
       "       'state', 'statement', 'static', 'status', 'std', 'step', 'stop',\n",
       "       'store', 'str', 'stream', 'string', 'struct', 'structure', 'stuff',\n",
       "       'style', 'sub', 'submit', 'suggest', 'sum', 'super', 'support',\n",
       "       'sure', 'switch', 'tab', 'table', 'tag', 'target', 'task', 'tell',\n",
       "       'temp', 'template', 'test', 'text', 'thing', 'think', 'thread',\n",
       "       'throw', 'time', 'title', 'token', 'tool', 'tostre', 'total',\n",
       "       'trigger', 'true', 'try', 'txt', 'type', 'understand', 'update',\n",
       "       'url', 'use', 'user', 'username', 'utf', 'val', 'valid', 'value',\n",
       "       'var', 'variable', 'vector', 'version', 'view', 'void', 'want',\n",
       "       'way', 'web', 'width', 'win', 'window', 'word', 'work', 'wrap',\n",
       "       'write', 'wrong', 'www', 'xml', 'xsl', 'year', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf model to find top 500 frequent words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "my_file = open(\"stackoverflow.txt\", \"r\") \n",
    "data = my_file.read() \n",
    "data_into_list = data.split(\"\\n\") \n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 500, stop_words = 'english')\n",
    "X = vectorizer.fit_transform(data_into_list)\n",
    "common = vectorizer.get_feature_names_out()\n",
    "\n",
    "# print out the frequent words\n",
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9822755-c1bd-4df6-8d61-5cbdf30c4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the word list to 'Top 500.txt' file\n",
    "with open('Top 500.txt', 'w') as f:\n",
    "    for line in common:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57b9389-b063-4212-b396-0fcc8ef2e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(\"./stackoverflow.txt\").filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890979f9-60ca-45c5-8186-22a3c7bf44e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FilterDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28760690-5cec-4da7-a161-9375b45c58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 5096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "\n",
    "def custom_standardization1(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization1,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8af29b-ae08-4e7d-a12e-3342f8970dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe4ca80-2062-44bc-b8b0-4b2ea065a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exec', 'sys', 'minute', 'protocol', 'live', 'timeout', 'invalid', 'student', 'nbsp', 'native', 'graph', 'city', 'complex', 'pos', 'quick', 'together', 'shouldn', 'hit', 'company', 'employee']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[1000:1020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c804dc7-251c-4047-95fa-4895995ccd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8943ddb-6141-4af0-b9d3-cab98596fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ed3c79-2e73-425c-8f64-37a681a7d551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnbatchDataset element_spec=TensorSpec(shape=(10,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a322521b-eabc-4829-86e7-59c99a02f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18366634\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1194a27e-870a-4d9c-b696-c98341107ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  39  188    3   75    6 2059  322 1233    2  348] => ['one', 'thing', 'you', 'find', 'that', 'selenium', 'seem', 'restart', 'the', 'browser']\n",
      "[   7    1  596 2580   23    2  348 1096  571    2] => ['use', '[UNK]', 'far', 'effective', 'but', 'the', 'browser', 'selection', 'limit', 'the']\n",
      "[   1    4 1695 1299  163    5  103  376 2567  123] => ['[UNK]', 'and', 'hopefully', 'temporary', 'solution', 'this', 'problem', 'python', 'cgi', 'script']\n",
      "[930 653 376   0   0   0   0   0   0   0] => ['usr', 'bin', 'python', '', '', '', '', '', '', '']\n",
      "[2567  123  922 3350 1440  291  397 1970 2355    0] => ['cgi', 'script', 'produce', 'rss', 'feed', 'top', 'level', 'gallery', 'album', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[990:995]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ad52e2-5a45-4703-9aec-d6ccf221e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c68aa8-6024-45e5-aa20-24dd33bead4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18366634/18366634 [45:37<00:00, 6708.81it/s]  \n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b468622a-0de7-402e-bc3d-3de584d227a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33258821"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b95d96ea-018f-4ed4-ab1c-a1b4bb31527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (33258821,)\n",
      "contexts.shape: (33258821, 5)\n",
      "labels.shape: (33258821, 5)\n"
     ]
    }
   ],
   "source": [
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b104675-afda-4559-be36-bc6c57ae07e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eeed378-f4e4-40cb-877b-43efee195322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aac323eb-0ae8-4f60-81cf-e29dad49c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57668f0a-b0ea-43e3-80fe-c54a81133868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2802cff0-8988-4e51-86c2-9ec35ba3621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "num_ns = 4\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "checkpoint_filepath = './checkpoint.model.keras'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a96f989-d6d6-4c43-9497-b1c2a490bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3644227-105b-4fb9-b85e-cf2a17972563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32479/32479 [==============================] - 143s 4ms/step - loss: 1.0570 - accuracy: 0.5772\n",
      "Epoch 2/30\n",
      "32479/32479 [==============================] - 142s 4ms/step - loss: 1.0015 - accuracy: 0.6025\n",
      "Epoch 3/30\n",
      "32479/32479 [==============================] - 142s 4ms/step - loss: 0.9926 - accuracy: 0.6062\n",
      "Epoch 4/30\n",
      "32479/32479 [==============================] - 143s 4ms/step - loss: 0.9886 - accuracy: 0.6078\n",
      "Epoch 5/30\n",
      "32479/32479 [==============================] - 143s 4ms/step - loss: 0.9865 - accuracy: 0.6086\n",
      "Epoch 6/30\n",
      "32479/32479 [==============================] - 143s 4ms/step - loss: 0.9852 - accuracy: 0.6092\n",
      "Epoch 7/30\n",
      "32479/32479 [==============================] - 143s 4ms/step - loss: 0.9843 - accuracy: 0.6095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3ddab1660>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=30, callbacks=[tensorboard_callback,model_checkpoint_callback, earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29f025fa-8418-4276-8a57-02e015ad0cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08cad63f-58da-474e-8572-7685bba68778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3603), started 0:52:29 ago. (Use '!kill 3603' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4caf3044b79fb94\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4caf3044b79fb94\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8553c363-2eeb-432d-89ab-18878ee79265",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f3fb989-577a-4bca-91ea-716f2f6a7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cda82-93f0-4ae4-bfb0-31b27fc9ef07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
