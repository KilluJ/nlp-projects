{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9332113-653a-4579-8dfe-3bc467819dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae80bed-0c68-4458-b5c3-5a3b6806840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    input_data = input_data.lower()\n",
    "    input_data = re.sub(r'^.*?<p>', '<p>', input_data)\n",
    "    input_data = re.sub(r'<p>', '', input_data)\n",
    "    input_data = re.sub(r'</p>', '', input_data)\n",
    "    input_data = re.sub(r'</a>', '', input_data)\n",
    "    input_data = re.sub(r'.*\\<(.*)\\>.*', '<a>', input_data)\n",
    "    input_data = re.sub(r'<a>', '', input_data)\n",
    "    #input_data = re.sub(r'\\n', '', input_data)\n",
    "    input_data = re.sub(r'<pre><code>', '', input_data)\n",
    "    input_data = re.sub(r'</code></pre>', '', input_data)\n",
    "    input_data = re.sub(r'<br>', '', input_data)\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    #stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    #return lowercase\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242e516-d357-4b5f-836e-e8180048c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(\"./\", \"stackoverflow.txt\"), 'w', encoding='utf-8')\n",
    "for line in open('Answers.csv', encoding = \"ISO-8859-1\"):\n",
    "    line = custom_standardization(line)\n",
    "    if line:\n",
    "        f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e57b9389-b063-4212-b396-0fcc8ef2e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(\"./stackoverflow.txt\").filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "890979f9-60ca-45c5-8186-22a3c7bf44e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FilterDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28760690-5cec-4da7-a161-9375b45c58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 5096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8af29b-ae08-4e7d-a12e-3342f8970dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abe4ca80-2062-44bc-b8b0-4b2ea065a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dictionary', 'validation', 'together', 'named', 'ex', 'ios', 'initialize', 'days', 'eclipse', 'special', 'removed', 'browsers', 'longer', 'shared', 'future', 'sent', 'shouldnt', 'visible', 'num', 'unsigned']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[1000:1020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c804dc7-251c-4047-95fa-4895995ccd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8943ddb-6141-4af0-b9d3-cab98596fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a322521b-eabc-4829-86e7-59c99a02f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18648272\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1194a27e-870a-4d9c-b696-c98341107ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0] => ['', '', '', '', '', '', '', '', '', '']\n",
      "[ 179  342   12   87 2800  482  139  734    5 1128] => ['another', 'option', 'i', 'was', 'considering', 'rather', 'than', 'writing', 'a', 'native']\n",
      "[  59    5    1   15 4498  514 1082  465  432    8] => ['its', 'a', '[UNK]', 'that', 'sun', 'never', 'included', 'anything', 'similar', 'in']\n",
      "[0 0 0 0 0 0 0 0 0 0] => ['', '', '', '', '', '', '', '', '', '']\n",
      "[364 452  93 432  56 118  12  24   5 241] => ['ive', 'got', 'something', 'similar', 'set', 'up', 'i', 'have', 'a', 'main']\n",
      "[1 1 0 0 0 0 0 0 0 0] => ['[UNK]', '[UNK]', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[999:1005]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00ad52e2-5a45-4703-9aec-d6ccf221e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09c68aa8-6024-45e5-aa20-24dd33bead4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18648272/18648272 [30:38<00:00, 10143.00it/s] \n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b95d96ea-018f-4ed4-ab1c-a1b4bb31527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (17396955,)\n",
      "contexts.shape: (17396955, 5)\n",
      "labels.shape: (17396955, 5)\n"
     ]
    }
   ],
   "source": [
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b104675-afda-4559-be36-bc6c57ae07e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2eeed378-f4e4-40cb-877b-43efee195322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aac323eb-0ae8-4f60-81cf-e29dad49c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57668f0a-b0ea-43e3-80fe-c54a81133868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2802cff0-8988-4e51-86c2-9ec35ba3621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "num_ns = 4\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "checkpoint_filepath = './checkpoint.model.keras'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.005, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a96f989-d6d6-4c43-9497-b1c2a490bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3644227-105b-4fb9-b85e-cf2a17972563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16989/16989 [==============================] - 118s 7ms/step - loss: 1.0470 - accuracy: 0.5871\n",
      "Epoch 2/20\n",
      "16989/16989 [==============================] - 102s 6ms/step - loss: 0.9630 - accuracy: 0.6240\n",
      "Epoch 3/20\n",
      "16989/16989 [==============================] - 104s 6ms/step - loss: 0.9401 - accuracy: 0.6328\n",
      "Epoch 4/20\n",
      "16989/16989 [==============================] - 105s 6ms/step - loss: 0.9270 - accuracy: 0.6375\n",
      "Epoch 5/20\n",
      "16989/16989 [==============================] - 105s 6ms/step - loss: 0.9187 - accuracy: 0.6405\n",
      "Epoch 6/20\n",
      "16989/16989 [==============================] - 103s 6ms/step - loss: 0.9130 - accuracy: 0.6425\n",
      "Epoch 7/20\n",
      "16989/16989 [==============================] - 104s 6ms/step - loss: 0.9088 - accuracy: 0.6440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0xa0230db10>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback,model_checkpoint_callback, earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29f025fa-8418-4276-8a57-02e015ad0cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08cad63f-58da-474e-8572-7685bba68778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 62463), started 1 day, 2:28:05 ago. (Use '!kill 62463' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-34ecb6a646e75f5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-34ecb6a646e75f5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8553c363-2eeb-432d-89ab-18878ee79265",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f3fb989-577a-4bca-91ea-716f2f6a7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cda82-93f0-4ae4-bfb0-31b27fc9ef07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
